#summary Groovy scripts

<h1>Introduction</h1>

This assumes that you have installed [http://groovy.codehaus.org Groovy] and that you have the command `groovy` available in your path. On Debian/Ubuntu systems, installing Groovy should be as easy as `apt-get install groovy`.

Copy one of the scripts into a simple text file (e.g. `pipeline`). Make the file executable (e.g. `chmod +x pipeline`). Then run it (e.g. `./pipeline`). The first time this will take quite long because libraries and models are automatically downloaded.

*Table of contents*

<wiki:toc max_depth="2" />

= Recipes = 

== OpenNLP Part-of-speech tagging pipeline writing to IMS Open Corpus Workbench format ==

Reads all text files (`*.txt`) in the specified folder and writes to the specified file.

Call with `pipeline <foldername> <language> <outputfile>`, e.g. `pipeline myFolder en output.tsv`.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.imscwb-asl', 
      version='1.5.0')

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.factory.CollectionReaderFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;

import de.tudarmstadt.ukp.dkpro.core.opennlp.*;
import de.tudarmstadt.ukp.dkpro.core.io.text.*;
import de.tudarmstadt.ukp.dkpro.core.io.imscwb.*;

// Assemble and run pipeline
runPipeline(  
  createReaderDescription(TextReader,
    TextReader.PARAM_PATH, args[0], // first command line parameter
    TextReader.PARAM_LANGUAGE, args[1], // second command line parameter
    TextReader.PARAM_PATTERNS, "[+]*.txt"),
  createEngineDescription(OpenNlpSegmenter),
  createEngineDescription(OpenNlpPosTagger),
  createEngineDescription(ImsCwbWriter,
    ImsCwbWriter.PARAM_TARGET_LOCATION, args[2])); // third command line parameter
}}}

== OpenNLP Part-of-speech tagging pipeline using custom writer component ==

Reads all text files (`*.txt`) in the specified folder and prints part-of-speech tags, one per line.

Call with `pipeline <foldername> <language>`, e.g. `pipeline myFolder en`.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl', 
      version='1.5.0')

import org.apache.uima.jcas.JCas;

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.util.JCasUtil.*;
import static org.apache.uima.fit.factory.CollectionReaderFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;

import de.tudarmstadt.ukp.dkpro.core.opennlp.*;
import de.tudarmstadt.ukp.dkpro.core.io.text.*;
import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.*;

// Assemble and run pipeline
runPipeline(  
  createReaderDescription(TextReader,
    TextReader.PARAM_PATH, args[0], // first command line parameter
    TextReader.PARAM_LANGUAGE, args[1], // second command line parameter
    TextReader.PARAM_PATTERNS, "[+]*.txt"),
  createEngineDescription(OpenNlpSegmenter),
  createEngineDescription(OpenNlpPosTagger),
  createEngineDescription(Writer));

// Custom writer class used at the end of the pipeline to write results to screen
class Writer extends org.apache.uima.fit.component.JCasAnnotator_ImplBase {
  void process(JCas jcas) {
    select(jcas, Token).each { println "${it.coveredText} ${it.pos.posValue}" }
  }
}
}}}

Example output:

{{{
The DT
quick JJ
brown JJ
fox NN
jumps NNS
over IN
the DT
lazy JJ
dog NN
. .
}}}

== OpenNLP Part-of-speech tagging pipeline with direct access to results ==

Reads all text files (`*.txt`) in the specified folder and prints part-of-speech tags, one per line.

Call with `pipeline <foldername> <language>`, e.g. `pipeline myFolder en`.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl', 
      version='1.5.0')

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.util.JCasUtil.*;
import static org.apache.uima.fit.factory.CollectionReaderFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;

import de.tudarmstadt.ukp.dkpro.core.opennlp.*;
import de.tudarmstadt.ukp.dkpro.core.io.text.*;
import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.*;
import de.tudarmstadt.ukp.dkpro.core.api.syntax.type.*;

def pipeline = iteratePipeline(
  createReaderDescription(TextReader,
    TextReader.PARAM_PATH, args[0],
    TextReader.PARAM_LANGUAGE, args[1],
    TextReader.PARAM_PATTERNS, "[+]*.txt"),
  createEngineDescription(OpenNlpSegmenter),
  createEngineDescription(OpenNlpPosTagger));

for (def jcas : pipeline) {
  select(jcas, Token).each { println "${it.coveredText} ${it.pos.posValue}" }
}
}}}

Example output:

{{{
The DT
quick JJ
brown JJ
fox NN
jumps NNS
over IN
the DT
lazy JJ
dog NN
. .
}}}

== OpenNLP Part-of-speech tagging & parsing without reader ==

This pipeline internally creates data, processes it, and writes results to the console.

Mind to provide more memory to Groovy using the command `export JAVA_OPTS="-Xmx1g"` before trying to run this.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0')

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.util.JCasUtil.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;
import org.apache.uima.fit.factory.JCasFactory;

import de.tudarmstadt.ukp.dkpro.core.opennlp.*;
import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.*;
import de.tudarmstadt.ukp.dkpro.core.api.syntax.type.*;

def jcas = JCasFactory.createJCas();
jcas.documentText = "This is a test";
jcas.documentLanguage = "en";

runPipeline(jcas,
  createEngineDescription(OpenNlpSegmenter),
  createEngineDescription(OpenNlpPosTagger),
  createEngineDescription(OpenNlpParser,
    OpenNlpParser.PARAM_WRITE_PENN_TREE, true));

select(jcas, Token).each { println "${it.coveredText} ${it.pos.posValue}" }

select(jcas, PennTree).each { println it.pennTree }
}}}

Example output:

{{{
This DT
is VBZ
a DT
test NN
(ROOT (S (NP (DT This)) (VP (VBZ is) (NP (DT a) (NN test)))))
}}}

== !MaltParser dependency parsing pipeline with direct access to results ==

Reads the specified file and prints dependencies, one per line. Multiple files can be specified using a wildcard, e.g. '*.txt' (the single quotes are part of the argument to avoid the shell expanding the wildcard!).

Call with `pipeline <foldername> <language>`, e.g. `pipeline myFolder en`.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.maltparser-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.conll-asl', 
      version='1.5.0')

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.util.JCasUtil.*;
import static org.apache.uima.fit.factory.CollectionReaderFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;

import de.tudarmstadt.ukp.dkpro.core.opennlp.*;
import de.tudarmstadt.ukp.dkpro.core.maltparser.*;
import de.tudarmstadt.ukp.dkpro.core.io.text.*;
import de.tudarmstadt.ukp.dkpro.core.api.syntax.type.dependency.*;

// Assemble and run pipeline
def pipeline = iteratePipeline(
  createReaderDescription(TextReader,
    TextReader.PARAM_SOURCE_LOCATION, args[0], // first command line parameter
    TextReader.PARAM_LANGUAGE, args[1]), // second command line parameter
  createEngineDescription(OpenNlpSegmenter),
  createEngineDescription(OpenNlpPosTagger),
  createEngineDescription(MaltParser));

for (def jcas : pipeline) {
  select(jcas, Dependency).each { 
    println "dep: [${it.dependencyType}] \t gov: [${it.governor.coveredText}] \t dep: [${it.dependent.coveredText}]" 
  }
}
}}}

Example output:

{{{
dep: [nsubj] 	 gov: [wants] 	 dep: [Alice]
dep: [aux] 	 gov: [see] 	 dep: [to]
dep: [xcomp] 	 gov: [wants] 	 dep: [see]
dep: [nn] 	 gov: [parsing] 	 dep: [dependency]
dep: [dobj] 	 gov: [see] 	 dep: [parsing]
dep: [prep] 	 gov: [parsing] 	 dep: [with]
dep: [pobj] 	 gov: [with] 	 dep: [UIMA.]
}}}

== !MaltParser dependency parsing pipeline writing to CONLL format ==

Reads all text files (`*.txt`) in the specified folder and prints dependencies, one per line.

Call with `pipeline <inputfolder> <language> <outputfolder>`, e.g. `pipeline input en output`.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.stanfordnlp-gpl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.maltparser-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.conll-asl', 
      version='1.5.0')

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.factory.CollectionReaderFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;

import de.tudarmstadt.ukp.dkpro.core.stanfordnlp.*;
import de.tudarmstadt.ukp.dkpro.core.maltparser.*;
import de.tudarmstadt.ukp.dkpro.core.io.conll.*;
import de.tudarmstadt.ukp.dkpro.core.io.text.*;

// Assemble and run pipeline
runPipeline(
  createReaderDescription(TextReader,
    TextReader.PARAM_PATH, args[0], // first command line parameter
    TextReader.PARAM_LANGUAGE, args[1], // second command line parameter
    TextReader.PARAM_PATTERNS, "[+]*.txt"),
  createEngineDescription(StanfordSegmenter),
  createEngineDescription(StanfordPosTagger),
  createEngineDescription(MaltParser),
  createEngineDescription(Conll2006Writer,
    Conll2006Writer.PARAM_TARGET_LOCATION, args[2])); // third command line parameter);
}}}

Example output:

{{{
1	The	_	DT	DT	_	4	det	_	_
2	quick	_	JJ	JJ	_	4	amod	_	_
3	brown	_	JJ	JJ	_	4	amod	_	_
4	fox	_	NN	NN	_	5	nsubj	_	_
5	jumps	_	VBZ	VBZ	_	0	_	_	_
6	over	_	IN	IN	_	5	prep	_	_
7	the	_	DT	DT	_	9	det	_	_
8	lazy	_	JJ	JJ	_	9	amod	_	_
9	dog	_	NN	NN	_	6	pobj	_	_
10	.	_	.	.	_	5	punct	_	_
}}}

== Fully mixed pipeline ==

Reads all text files (`*.txt`) in the specified folder and prints token, part-of-speech, lemma, and named entity in a tab-separated format, followed by the constituent tree as a bracketed structure.

Call with `pipeline <inputfolder> <language>`, e.g. `pipeline input en`.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.matetools-gpl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.clearnlp-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.berkeleyparser-gpl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.stanfordnlp-gpl', 
      version='1.5.0')

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.util.JCasUtil.*;
import static org.apache.uima.fit.factory.CollectionReaderFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;

import de.tudarmstadt.ukp.dkpro.core.io.text.*;
import de.tudarmstadt.ukp.dkpro.core.opennlp.*;
import de.tudarmstadt.ukp.dkpro.core.matetools.*;
import de.tudarmstadt.ukp.dkpro.core.clearnlp.*;
import de.tudarmstadt.ukp.dkpro.core.berkeleyparser.*;
import de.tudarmstadt.ukp.dkpro.core.stanfordnlp.*;
import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.*;
import de.tudarmstadt.ukp.dkpro.core.api.ner.type.*;
import de.tudarmstadt.ukp.dkpro.core.api.syntax.type.*;

// Assemble and run pipeline
def pipeline = iteratePipeline(
  createReaderDescription(TextReader,
    TextReader.PARAM_PATH, args[0],     //first command line parameter
    TextReader.PARAM_LANGUAGE, args[1], //second command line parameter
    TextReader.PARAM_PATTERNS, "[+]*.txt"),
  createEngineDescription(OpenNlpSegmenter),
  createEngineDescription(MatePosTagger),
  createEngineDescription(ClearNlpLemmatizer),
  createEngineDescription(BerkeleyParser,
    BerkeleyParser.PARAM_WRITE_PENN_TREE, true),
  createEngineDescription(StanfordNamedEntityRecognizer));

for (def jcas : pipeline) {
  for (def sentence : select(jcas, Sentence)) {
    println "Token\tPOS\tLemma\tNamed entity";
    selectCovered(Token, sentence).each {
      print "${it.coveredText}\t${it.pos.posValue}\t${it.lemma.value}\t";
      print "${selectCovering(NamedEntity, it)*.value}\n";
    };
    selectCovered(PennTree, sentence).each {println "\n${it.pennTree}"};
  }
} 
}}}

Example output:

{{{
Token	POS	Lemma	Named entity
Jim	NNP	jim	[PERSON]
bought	VBD	buy	[]
300	CD	0	[]
shares	NNS	share	[]
of	IN	of	[]
Acme	NNP	acme	[ORGANIZATION]
Corp.	NNP	corp.	[ORGANIZATION]
in	IN	in	[]
2006	CD	0	[]
.	.	.	[]

(ROOT (S (NP (NNP Jim)) (VP (VBD bought) (NP (NP (CD 300) (NNS shares))
(PP (IN of) (NP (NNP Acme) (NNP Corp.)))) (PP (IN in) (NP (CD 2006)))) (. .)))
}}}

== Segmentation evaluator ==

Reads a test suite from an XML file and tries it against several segmenters.

Call with `pipeline <filename>`, e.g. `pipeline testsuite.xml`.

Instead of specifying a filename, it is also possible to use an URL, e.g.:

`pipeline http://lalala`

which runs the set of segmentation tests maintained [DKProSegmentationTestSuite here].

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core',
      module='de.tudarmstadt.ukp.dkpro.core.tokit-asl',
      version='1.5.0')
import de.tudarmstadt.ukp.dkpro.core.tokit.*

@Grab(group='de.tudarmstadt.ukp.dkpro.core',
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl',
      version='1.5.0')
import de.tudarmstadt.ukp.dkpro.core.opennlp.*

@Grab(group='de.tudarmstadt.ukp.dkpro.core',
      module='de.tudarmstadt.ukp.dkpro.core.languagetool-asl',
      version='1.5.0')
import de.tudarmstadt.ukp.dkpro.core.languagetool.*

@Grab(group='de.tudarmstadt.ukp.dkpro.core',
      module='de.tudarmstadt.ukp.dkpro.core.stanfordnlp-gpl',
      version='1.5.0')
import de.tudarmstadt.ukp.dkpro.core.stanfordnlp.*

import static org.apache.uima.fit.util.JCasUtil.*
import static org.apache.uima.fit.factory.JCasFactory.*
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*
import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.*

java.util.logging.LogManager.logManager.reset(); // Disable logging
def testSuite = new XmlSlurper().parse(args[0]) // Load tests
def doc = createJCas() // Create empty reusable document

// These are the segmenters to test
def segmenters = [ BreakIteratorSegmenter, StanfordSegmenter, 
  OpenNlpSegmenter, LanguageToolSegmenter ]

def results = [:];
for (def tc : testSuite.tests.test) { // Run tests
  println "=== ${tc.@id} ===\n"

  for (def segmenter : segmenters) { // Test each segmenter in turn
    // Process test input
    doc.reset()
    doc.documentText = tc.input
    doc.documentLanguage = tc.@language
    createEngine(segmenter).process(doc)

    // Construct comparable output
    def output = select(doc, Sentence).collect {s ->
      "◀" + selectCovered(Token, s).collect { "⁌${it.coveredText}⁍ " }.join("") + "▶ "
    }.join("")

    // Find output in test suite
    def diag = "unknown"
    for (def possibleOutput : tc.possibleOutputs.possibleOutput) {
      if (output.trim() == possibleOutput.output.text().trim()) {
        diag = possibleOutput.description
        break
      }
    }
    
    // Print result for individual segmenter
    println "Segmenter : ${segmenter.simpleName}" 
    println "Output    : ${output}"
    println "Result    : ${diag}"
    println ""

    if (results[segmenter] == null) results[segmenter] = [];
    results[segmenter] << diag;
  }
}

// Print results overview
for (def segmenter : segmenters) {
  println "${segmenter.simpleName}"
  println "  GOOD: ${results[segmenter].grep(~/GOOD.*/).size()}"
  println "  BAD : ${results[segmenter].grep(~/BAD.*/).size()}"
}
}}}

Example test suite:

{{{
<testSuite>
  <tests>
    <test id="number.version.underspecified" language="en">
      <input>John loves Windows 3.x and DOS.</input>
      <possibleOutputs>
        <possibleOutput>
          <output>◀⁌John⁍ ⁌loves⁍ ⁌Windows⁍ ⁌3⁍ ⁌.⁍ ⁌x⁍ ⁌and⁍ ⁌DOS⁍ ⁌.⁍ ▶</output>
          <description>BAD - version split into multiple tokens</description>
        </possibleOutput>
        <possibleOutput>
          <output>◀⁌John⁍ ⁌loves⁍ ⁌Windows⁍ ⁌3.x⁍ ⁌and⁍ ⁌DOS.⁍ ▶</output>
          <description>BAD - sentence marker is not a separate token</description>
        </possibleOutput>
      </possibleOutputs>
    </test>
  </tests>
</testSuite>
}}}

Example output:

{{{
=== number.version.underspecified ===

Segmenter : BreakIteratorSegmenter
Output    : ◀⁌John⁍ ⁌loves⁍ ⁌Windows⁍ ⁌3⁍ ⁌.⁍ ⁌x⁍ ⁌and⁍ ⁌DOS⁍ ⁌.⁍ ▶
Result    : BAD - version split into multiple tokens

Segmenter : StanfordSegmenter
Output    : ◀⁌John⁍ ⁌loves⁍ ⁌Windows⁍ ⁌3⁍ ⁌.⁍ ▶ ◀⁌x⁍ ⁌and⁍ ⁌DOS⁍ ⁌.⁍ ▶
Result    : unknown

Segmenter : OpenNlpSegmenter
Output    : ◀⁌John⁍ ⁌loves⁍ ⁌Windows⁍ ⁌3.x⁍ ⁌and⁍ ⁌DOS.⁍ ▶
Result    : BAD - sentence marker is not a separate token

Segmenter : LanguageToolSegmenter
Output    : ◀⁌John⁍ ⁌loves⁍ ⁌Windows⁍ ⁌3⁍ ⁌.⁍ ⁌x⁍ ⁌and⁍ ⁌DOS⁍ ⁌.⁍ ▶
Result    : BAD - version split into multiple tokens

BreakIteratorSegmenter
  GOOD: 0
  BAD : 1
StanfordSegmenter
  GOOD: 0
  BAD : 0
OpenNlpSegmenter
  GOOD: 0
  BAD : 1
LanguageToolSegmenter
  GOOD: 0
  BAD : 1
}}}

= Trouble shooting = 

== Out of memory ==

If a script complains about not having enough heap, try the command 
{{{
Linux:    export JAVA_OPTS="-Xmx1g"
}}} 

and then run the script again.

== Verbose dependency resolving ==

Normally, grape resolves dependencies quietly. If a script has many dependencies, that can mean the script may be running for a long time without any visible output on screen, looking like it is hanging. What it really does is downloading the dependencies. To enable verbose output during the dependency resolving phase, set JAVA_OPTS:

{{{
Linux:    export JAVA_OPTS="-Dgroovy.grape.report.downloads=true $JAVA_OPTS"
Windows:  set JAVA_OPTS="-Dgroovy.grape.report.downloads=true %JAVA_OPTS%"
}}}

== Flush caches ==

The scripts download required models and libraries automatically. Sometimes it may be necessary to flush the cache folders. Try deleting `~/.groovy/grapes` and `~/.ivy2/cache`. If you did not separate the Groovy cache from the Maven cache, you might also want to consider deleting `~/.m2/repository`. If you use Maven for software development, we recommend that you separate the caches.

== Separate Groovy Grape Cache from Maven Cache ==

On some systems, Groovy per default re-uses artifacts that have already been downloaded by Maven. To make sure the Groovy Grape cache is fully separate from the Maven cache, create a file called `grapeConfig.xml` in your `~\.groovy` folder with this content

{{{
<?xml version="1.0"?>
<ivysettings>
    <settings defaultResolver="downloadGrapes"/>
    <resolvers>
        <chain name="downloadGrapes">
            <!-- todo add 'endorsed groovy extensions' resolver here -->
            <filesystem name="cachedGrapes">
                <ivy pattern="${user.home}/.groovy/grapes/[organisation]/[module]/ivy-[revision].xml"/>
                <artifact pattern="${user.home}/.groovy/grapes/[organisation]/[module]/[type]s/[artifact]-[revision].[ext]"/>
            </filesystem>
            <ibiblio name="codehaus" root="http://repository.codehaus.org/" m2compatible="true"/>
            <ibiblio name="ibiblio" m2compatible="true"/>
            <ibiblio name="java.net2" root="http://download.java.net/maven/2/" m2compatible="true"/>
        </chain>
    </resolvers>
</ivysettings>
}}}

Then flush the cache by deleting the folder `~/.groovy/grapes`. Mind that the next time you run a Groovy script, it will take some more time, because the cache needs to be repopulated.

== Using SNAPSHOT versions ==

To use SNAPSHOT versions of DKPro Core, add the following line before the first `@Grab` line:

{{{
@GrabResolver(name='ukp-oss-snapshots',
      root='http://zoidberg.ukp.informatik.tu-darmstadt.de/artifactory/public-snapshots')
}}}

Then change the versions of all DKPro Core components to the SNAPSHOT version that you wish to use. It is strongly recommended not to mix versions!