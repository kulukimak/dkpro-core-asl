#summary Groovy scripts

<h1>Introduction</h1>

This assumes that you have installed [http://groovy.codehaus.org Groovy] and that you have the command `groovy` available in your path. On Debian/Ubuntu systems, installing Groovy should be as easy as `apt-get install groovy`.

*Note: An [http://stackoverflow.com/questions/23504261/dkpro-groovy-usage-and-installation-with-uima incompatibility] has been reported with Groovy version 2.3.0. Please use a 2.1.x or 2.2.x version. (See also: [http://jira.codehaus.org/browse/GROOVY-6768 GROOVY-6768])*

Copy one of the scripts into a simple text file (e.g. `pipeline`). Make the file executable (e.g. `chmod +x pipeline`). Then run it (e.g. `./pipeline`). The first time this will take quite long because libraries and models are automatically downloaded.

*Table of contents*

<wiki:toc max_depth="2" />

= Recipes = 

== Some simple things you can do with DKPro Core ==

In this section, we pick up some of the [http://www.nltk.org NLTK examples] and implement them using DKPro Core.

=== Tokenize and tag some text ===

Original NLTK example:

{{{
>>> import nltk
>>> sentence = """At eight o'clock on Thursday morning
... Arthur didn't feel very good."""
>>> tokens = nltk.word_tokenize(sentence)
>>> tokens
['At', 'eight', "o'clock", 'on', 'Thursday', 'morning',
'Arthur', 'did', "n't", 'feel', 'very', 'good', '.']
>>> tagged = nltk.pos_tag(tokens)
>>> tagged[0:6]
[('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'), ('on', 'IN'),
('Thursday', 'NNP'), ('morning', 'NN')]
}}}

DKPro Core Groovy version:

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', version='1.5.0',
      module='de.tudarmstadt.ukp.dkpro.core.stanfordnlp-gpl')
import de.tudarmstadt.ukp.dkpro.core.stanfordnlp.*;
@Grab(group='de.tudarmstadt.ukp.dkpro.core', version='1.5.0',
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl')
import de.tudarmstadt.ukp.dkpro.core.io.text.*;

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.factory.CollectionReaderFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;
import static org.apache.uima.fit.util.JCasUtil.*;

import de.tudarmstadt.ukp.dkpro.core.io.text.*;
import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.*;

def sentence = """At eight o'clock on Thursday morning 
Arthur didn't feel very good."""

def result = iteratePipeline(
  createReaderDescription(StringReader,
    StringReader.PARAM_DOCUMENT_TEXT, sentence,
    StringReader.PARAM_LANGUAGE, "en"),
  createEngineDescription(StanfordSegmenter),
  createEngineDescription(StanfordPosTagger));

result.each { println select(it, Token).collect { it.coveredText } }

result.each { doc ->
  println select(doc, Token).collect { token ->
    [token.coveredText, token.pos.posValue ] }[0..5] }
}}}

Output: 

{{{
[At, eight, o'clock, on, Thursday, morning, Arthur, did, n't, feel, very, good, .]
[[At, IN], [eight, CD], [o'clock, RB], [on, IN], [Thursday, NNP], [morning, NN]]
}}}

=== Identify named entities ===

Original NLTK example (must be run immediately after the previous example):

{{{
>>> entities = nltk.chunk.ne_chunk(tagged)
>>> entities
Tree('S', [('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'),
           ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'),
       Tree('PERSON', [('Arthur', 'NNP')]),
           ('did', 'VBD'), ("n't", 'RB'), ('feel', 'VB'),
           ('very', 'RB'), ('good', 'JJ'), ('.', '.')])
}}}

DKPro Core Groovy version:

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', version='1.5.0',
      module='de.tudarmstadt.ukp.dkpro.core.stanfordnlp-gpl')
import de.tudarmstadt.ukp.dkpro.core.stanfordnlp.*;

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.factory.JCasFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;
import static org.apache.uima.fit.util.JCasUtil.*;

import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.*;
import de.tudarmstadt.ukp.dkpro.core.api.ner.type.*;

def doc = createJCas();
doc.documentText = """At eight o'clock on Thursday morning 
Arthur didn't feel very good."""
doc.documentLanguage = "en";

runPipeline(doc,
  createEngineDescription(StanfordSegmenter),
  createEngineDescription(StanfordPosTagger),
  createEngineDescription(StanfordNamedEntityRecognizer));

println select(doc, Token).collect { it.coveredText }

println select(doc, Token).collect { 
  [it.coveredText, it.pos.posValue ] }[0..5]

println select(doc, Token).collect { 
  [it.coveredText, selectCovering(NamedEntity, it).collect { it.value } ] }
}}}

Output:

{{{
[At, eight, o'clock, on, Thursday, morning, Arthur, did, n't, feel, very, good, .]
[[At, IN], [eight, CD], [o'clock, RB], [on, IN], [Thursday, NNP], [morning, NN]]
[[At, []], [eight, []], [o'clock, []], [on, []], [Thursday, []], [morning, []], [Arthur, [PERSON]], [did, []], [n't, []], [feel, []], [very, []], [good, []], [., []]]
}}}

=== Comparison NLTK vs. DKPro Core ===

We notice that the NLTK examples are much shorter, even though they include the output of the commands. The NTLK examples are run in an interactive Python shell where we just have to hack in a couple of commands. The DKPro Core examples are comparatively longish scripts - nothing that one would want to hack into a shell to play around with and explore NLP tools. 

With a single `import nltk` we get access to a lot of functionality in NLTK, e.g. a default tokenizer, tagger, named entity recognizer, etc. However, all of these are for English only. In DKPro Core we need to first add dependencies on all the modules using `@Grab`and then import the actual tools from the modules. We also need to add several unrelated imports to get access to necessary functions like `select`or `createEngineDescription`. 

In NLTK we only have convenient access to a few tools for English. The script does not know what version of NLTK it is supposed to run with. With DKPro Core, we have access to a wide array of integrated tools and we know exactly which version of each tool we use. Also most of the tools do not only support English, but also additional languages.

In NLTK we can nicely execute one analysis step after the other and always explore the intermediate results. The DKPro Core scripts are more suitable for the batch-processing of larger amounts of documents.

== OpenNLP Part-of-speech tagging pipeline writing to IMS Open Corpus Workbench format ==

Reads all text files (`*.txt`) in the specified folder and writes to the specified file.

Call with `pipeline <foldername> <language> <outputfile>`, e.g. `pipeline myFolder en output.tsv`.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.imscwb-asl', 
      version='1.5.0')

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.factory.CollectionReaderFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;

import de.tudarmstadt.ukp.dkpro.core.opennlp.*;
import de.tudarmstadt.ukp.dkpro.core.io.text.*;
import de.tudarmstadt.ukp.dkpro.core.io.imscwb.*;

// Assemble and run pipeline
runPipeline(  
  createReaderDescription(TextReader,
    TextReader.PARAM_PATH, args[0], // first command line parameter
    TextReader.PARAM_LANGUAGE, args[1], // second command line parameter
    TextReader.PARAM_PATTERNS, "[+]*.txt"),
  createEngineDescription(OpenNlpSegmenter),
  createEngineDescription(OpenNlpPosTagger),
  createEngineDescription(ImsCwbWriter,
    ImsCwbWriter.PARAM_TARGET_LOCATION, args[2])); // third command line parameter
}}}

== OpenNLP Part-of-speech tagging pipeline using custom writer component ==

Reads all text files (`*.txt`) in the specified folder and prints part-of-speech tags, one per line.

Call with `pipeline <foldername> <language>`, e.g. `pipeline myFolder en`.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl', 
      version='1.5.0')

import org.apache.uima.jcas.JCas;

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.util.JCasUtil.*;
import static org.apache.uima.fit.factory.CollectionReaderFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;

import de.tudarmstadt.ukp.dkpro.core.opennlp.*;
import de.tudarmstadt.ukp.dkpro.core.io.text.*;
import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.*;

// Assemble and run pipeline
runPipeline(  
  createReaderDescription(TextReader,
    TextReader.PARAM_PATH, args[0], // first command line parameter
    TextReader.PARAM_LANGUAGE, args[1], // second command line parameter
    TextReader.PARAM_PATTERNS, "[+]*.txt"),
  createEngineDescription(OpenNlpSegmenter),
  createEngineDescription(OpenNlpPosTagger),
  createEngineDescription(Writer));

// Custom writer class used at the end of the pipeline to write results to screen
class Writer extends org.apache.uima.fit.component.JCasAnnotator_ImplBase {
  void process(JCas jcas) {
    select(jcas, Token).each { println "${it.coveredText} ${it.pos.posValue}" }
  }
}
}}}

Example output:

{{{
The DT
quick JJ
brown JJ
fox NN
jumps NNS
over IN
the DT
lazy JJ
dog NN
. .
}}}

== OpenNLP Part-of-speech tagging pipeline with direct access to results ==

Reads all text files (`*.txt`) in the specified folder and prints part-of-speech tags, one per line.

Call with `pipeline <foldername> <language>`, e.g. `pipeline myFolder en`.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl', 
      version='1.5.0')

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.util.JCasUtil.*;
import static org.apache.uima.fit.factory.CollectionReaderFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;

import de.tudarmstadt.ukp.dkpro.core.opennlp.*;
import de.tudarmstadt.ukp.dkpro.core.io.text.*;
import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.*;
import de.tudarmstadt.ukp.dkpro.core.api.syntax.type.*;

def pipeline = iteratePipeline(
  createReaderDescription(TextReader,
    TextReader.PARAM_PATH, args[0],
    TextReader.PARAM_LANGUAGE, args[1],
    TextReader.PARAM_PATTERNS, "[+]*.txt"),
  createEngineDescription(OpenNlpSegmenter),
  createEngineDescription(OpenNlpPosTagger));

for (def jcas : pipeline) {
  select(jcas, Token).each { println "${it.coveredText} ${it.pos.posValue}" }
}
}}}

Example output:

{{{
The DT
quick JJ
brown JJ
fox NN
jumps NNS
over IN
the DT
lazy JJ
dog NN
. .
}}}

== OpenNLP Part-of-speech tagging & parsing without reader ==

This pipeline internally creates data, processes it, and writes results to the console.

Mind to provide more memory to Groovy using the command `export JAVA_OPTS="-Xmx1g"` before trying to run this.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0')

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.util.JCasUtil.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;
import org.apache.uima.fit.factory.JCasFactory;

import de.tudarmstadt.ukp.dkpro.core.opennlp.*;
import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.*;
import de.tudarmstadt.ukp.dkpro.core.api.syntax.type.*;

def jcas = JCasFactory.createJCas();
jcas.documentText = "This is a test";
jcas.documentLanguage = "en";

runPipeline(jcas,
  createEngineDescription(OpenNlpSegmenter),
  createEngineDescription(OpenNlpPosTagger),
  createEngineDescription(OpenNlpParser,
    OpenNlpParser.PARAM_WRITE_PENN_TREE, true));

select(jcas, Token).each { println "${it.coveredText} ${it.pos.posValue}" }

select(jcas, PennTree).each { println it.pennTree }
}}}

Example output:

{{{
This DT
is VBZ
a DT
test NN
(ROOT (S (NP (DT This)) (VP (VBZ is) (NP (DT a) (NN test)))))
}}}

== !MaltParser dependency parsing pipeline with direct access to results ==

Reads the specified file and prints dependencies, one per line. Multiple files can be specified using a wildcard, e.g. '*.txt' (the single quotes are part of the argument to avoid the shell expanding the wildcard!).

Call with `pipeline <foldername> <language>`, e.g. `pipeline myFolder en`.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.maltparser-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.conll-asl', 
      version='1.5.0')

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.util.JCasUtil.*;
import static org.apache.uima.fit.factory.CollectionReaderFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;

import de.tudarmstadt.ukp.dkpro.core.opennlp.*;
import de.tudarmstadt.ukp.dkpro.core.maltparser.*;
import de.tudarmstadt.ukp.dkpro.core.io.text.*;
import de.tudarmstadt.ukp.dkpro.core.api.syntax.type.dependency.*;

// Assemble and run pipeline
def pipeline = iteratePipeline(
  createReaderDescription(TextReader,
    TextReader.PARAM_SOURCE_LOCATION, args[0], // first command line parameter
    TextReader.PARAM_LANGUAGE, args[1]), // second command line parameter
  createEngineDescription(OpenNlpSegmenter),
  createEngineDescription(OpenNlpPosTagger),
  createEngineDescription(MaltParser));

for (def jcas : pipeline) {
  select(jcas, Dependency).each { 
    println "dep: [${it.dependencyType}] \t gov: [${it.governor.coveredText}] \t dep: [${it.dependent.coveredText}]" 
  }
}
}}}

Example output:

{{{
dep: [det] 	 gov: [jumps] 	 dep: [The]
dep: [amod] 	 gov: [jumps] 	 dep: [quick]
dep: [amod] 	 gov: [jumps] 	 dep: [brown]
dep: [nn] 	 gov: [jumps] 	 dep: [fox]
dep: [prep] 	 gov: [jumps] 	 dep: [over]
dep: [det] 	 gov: [dog] 	 dep: [the]
dep: [amod] 	 gov: [dog] 	 dep: [lazy]
dep: [pobj] 	 gov: [over] 	 dep: [dog]
dep: [punct] 	 gov: [jumps] 	 dep: [.]
}}}

== !MaltParser dependency parsing pipeline writing to CONLL format ==

Reads all text files (`*.txt`) in the specified folder and prints dependencies, one per line.

Call with `pipeline <inputfolder> <language> <outputfolder>`, e.g. `pipeline input en output`.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.stanfordnlp-gpl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.maltparser-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.conll-asl', 
      version='1.5.0')

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.factory.CollectionReaderFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;

import de.tudarmstadt.ukp.dkpro.core.stanfordnlp.*;
import de.tudarmstadt.ukp.dkpro.core.maltparser.*;
import de.tudarmstadt.ukp.dkpro.core.io.conll.*;
import de.tudarmstadt.ukp.dkpro.core.io.text.*;

// Assemble and run pipeline
runPipeline(
  createReaderDescription(TextReader,
    TextReader.PARAM_PATH, args[0], // first command line parameter
    TextReader.PARAM_LANGUAGE, args[1], // second command line parameter
    TextReader.PARAM_PATTERNS, "[+]*.txt"),
  createEngineDescription(StanfordSegmenter),
  createEngineDescription(StanfordPosTagger),
  createEngineDescription(MaltParser),
  createEngineDescription(Conll2006Writer,
    Conll2006Writer.PARAM_TARGET_LOCATION, args[2])); // third command line parameter);
}}}

Example output:

{{{
1	The	_	DT	DT	_	4	det	_	_
2	quick	_	JJ	JJ	_	4	amod	_	_
3	brown	_	JJ	JJ	_	4	amod	_	_
4	fox	_	NN	NN	_	5	nsubj	_	_
5	jumps	_	VBZ	VBZ	_	0	_	_	_
6	over	_	IN	IN	_	5	prep	_	_
7	the	_	DT	DT	_	9	det	_	_
8	lazy	_	JJ	JJ	_	9	amod	_	_
9	dog	_	NN	NN	_	6	pobj	_	_
10	.	_	.	.	_	5	punct	_	_
}}}

== Fully mixed pipeline ==

Reads all text files (`*.txt`) in the specified folder and prints token, part-of-speech, lemma, and named entity in a tab-separated format, followed by the constituent tree as a bracketed structure.

Call with `pipeline <inputfolder> <language>`, e.g. `pipeline input en`.

{{{
#!/usr/bin/env groovy
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.io.text-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.matetools-gpl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.clearnlp-asl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.berkeleyparser-gpl', 
      version='1.5.0')
@Grab(group='de.tudarmstadt.ukp.dkpro.core', 
      module='de.tudarmstadt.ukp.dkpro.core.stanfordnlp-gpl', 
      version='1.5.0')

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.util.JCasUtil.*;
import static org.apache.uima.fit.factory.CollectionReaderFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;

import de.tudarmstadt.ukp.dkpro.core.io.text.*;
import de.tudarmstadt.ukp.dkpro.core.opennlp.*;
import de.tudarmstadt.ukp.dkpro.core.matetools.*;
import de.tudarmstadt.ukp.dkpro.core.clearnlp.*;
import de.tudarmstadt.ukp.dkpro.core.berkeleyparser.*;
import de.tudarmstadt.ukp.dkpro.core.stanfordnlp.*;
import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.*;
import de.tudarmstadt.ukp.dkpro.core.api.ner.type.*;
import de.tudarmstadt.ukp.dkpro.core.api.syntax.type.*;

// Assemble and run pipeline
def pipeline = iteratePipeline(
  createReaderDescription(TextReader,
    TextReader.PARAM_PATH, args[0],     //first command line parameter
    TextReader.PARAM_LANGUAGE, args[1], //second command line parameter
    TextReader.PARAM_PATTERNS, "[+]*.txt"),
  createEngineDescription(OpenNlpSegmenter),
  createEngineDescription(MatePosTagger),
  createEngineDescription(ClearNlpLemmatizer),
  createEngineDescription(BerkeleyParser,
    BerkeleyParser.PARAM_WRITE_PENN_TREE, true),
  createEngineDescription(StanfordNamedEntityRecognizer));

for (def jcas : pipeline) {
  for (def sentence : select(jcas, Sentence)) {
    println "Token\tPOS\tLemma\tNamed entity";
    selectCovered(Token, sentence).each {
      print "${it.coveredText}\t${it.pos.posValue}\t${it.lemma.value}\t";
      print "${selectCovering(NamedEntity, it)*.value}\n";
    };
    selectCovered(PennTree, sentence).each {println "\n${it.pennTree}"};
  }
} 
}}}

Example output:

{{{
Token	POS	Lemma	Named entity
Jim	NNP	jim	[PERSON]
bought	VBD	buy	[]
300	CD	0	[]
shares	NNS	share	[]
of	IN	of	[]
Acme	NNP	acme	[ORGANIZATION]
Corp.	NNP	corp.	[ORGANIZATION]
in	IN	in	[]
2006	CD	0	[]
.	.	.	[]

(ROOT (S (NP (NNP Jim)) (VP (VBD bought) (NP (NP (CD 300) (NNS shares))
(PP (IN of) (NP (NNP Acme) (NNP Corp.)))) (PP (IN in) (NP (CD 2006)))) (. .)))
}}}

== Segmenter Evaluator ==

Reads a test suite from an XML file and tries it against several segmenters. The evaluator has moved [GroovySegmenterEvaluator here].

== Decompounding without a ranker resource ==

Uses the !LeftToRightSplitter as the splitter resource and no ranker resource, decompounds the compounds in a sentence after tokenizing it, then print the tokens and each compound part.


{{{

#!/usr/bin/env groovy

@GrabResolver(name='ukp-oss-releases',
     root='http://zoidberg.ukp.informatik.tu-darmstadt.de/artifactory/public-releases')

@GrabResolver(name='ukp-oss-snapshots',
     root='http://zoidberg.ukp.informatik.tu-darmstadt.de/artifactory/public-snapshots')

@Grab(group='de.tudarmstadt.ukp.dkpro.core', version='1.7.0-SNAPSHOT',
     module='de.tudarmstadt.ukp.dkpro.core.decompounding-asl')

@Grab(group='de.tudarmstadt.ukp.dkpro.core', version='1.7.0-SNAPSHOT',
     module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl')

@Grab(group='de.tudarmstadt.ukp.dkpro.core', version='20110621.1',
      module='de.tudarmstadt.ukp.dkpro.core.decompounding-model-spelling-de-igerman98')

import de.tudarmstadt.ukp.dkpro.core.decompounding.uima.annotator.*;
import de.tudarmstadt.ukp.dkpro.core.decompounding.uima.resource.*;

import de.tudarmstadt.ukp.dkpro.core.opennlp.*;
import de.tudarmstadt.ukp.dkpro.core.decompounding.uima.resource.*;

import static org.apache.uima.fit.pipeline.SimplePipeline.*;
import static org.apache.uima.fit.factory.JCasFactory.*;
import static org.apache.uima.fit.factory.AnalysisEngineFactory.*;
import static org.apache.uima.fit.factory.ExternalResourceFactory.*;
import static org.apache.uima.fit.util.JCasUtil.*;

import de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.*;


def doc = createJCas();
doc.documentText = "Wir brauchen einen Aktionsplan."
doc.documentLanguage = "de";

runPipeline(doc,
  createEngineDescription(OpenNlpSegmenter),
  createEngineDescription(
    CompoundAnnotator,
    CompoundAnnotator.PARAM_SPLITTING_ALGO,
      createExternalResourceDescription(
        LeftToRightSplitterResource,
        (Object) LeftToRightSplitterResource.PARAM_DICT_RESOURCE,
          createExternalResourceDescription(SharedDictionary),
        LeftToRightSplitterResource.PARAM_MORPHEME_RESOURCE,
          createExternalResourceDescription(SharedLinkingMorphemes))));

println select(doc, Token).collect { it.coveredText }

println select(doc, CompoundPart).collect { it.coveredText }


}}}

Example output:

{{{
Jul 02, 2014 4:52:49 PM de.tudarmstadt.ukp.dkpro.core.api.resources.ResourceObjectProviderBase info
Information: :: loading settings :: url = jar:file:/usr/share/groovy/lib/ivy.jar!/org/apache/ivy/core/settings/ivysettings.xml
Jul 02, 2014 4:52:49 PM de.tudarmstadt.ukp.dkpro.core.api.resources.ResourceObjectProviderBase configure
Information: Producing resource from [jar:file:/home/santos/.ivy2/cache/de.tudarmstadt.ukp.dkpro.core/de.tudarmstadt.ukp.dkpro.core.opennlp-upstream-sentence-de-maxent/jars/de.tudarmstadt.ukp.dkpro.core.opennlp-upstream-sentence-de-maxent-20120616.jar!/de/tudarmstadt/ukp/dkpro/core/opennlp/lib/sentence-de-maxent.bin] redirected from [jar:file:/home/santos/.ivy2/cache/de.tudarmstadt.ukp.dkpro.core/de.tudarmstadt.ukp.dkpro.core.opennlp-model-sentence-de-maxent/jars/de.tudarmstadt.ukp.dkpro.core.opennlp-model-sentence-de-maxent-20120616.1.jar!/de/tudarmstadt/ukp/dkpro/core/opennlp/lib/sentence-de-maxent.properties]
Jul 02, 2014 4:52:49 PM de.tudarmstadt.ukp.dkpro.core.api.resources.ResourceObjectProviderBase loadResource
Information: Producing resource took 55ms
Jul 02, 2014 4:52:49 PM de.tudarmstadt.ukp.dkpro.core.api.resources.ResourceObjectProviderBase info
Information: :: loading settings :: url = jar:file:/usr/share/groovy/lib/ivy.jar!/org/apache/ivy/core/settings/ivysettings.xml
Jul 02, 2014 4:52:50 PM de.tudarmstadt.ukp.dkpro.core.api.resources.ResourceObjectProviderBase configure
Information: Producing resource from [jar:file:/home/santos/.ivy2/cache/de.tudarmstadt.ukp.dkpro.core/de.tudarmstadt.ukp.dkpro.core.opennlp-upstream-token-de-maxent/jars/de.tudarmstadt.ukp.dkpro.core.opennlp-upstream-token-de-maxent-20120616.jar!/de/tudarmstadt/ukp/dkpro/core/opennlp/lib/token-de-maxent.bin] redirected from [jar:file:/home/santos/.ivy2/cache/de.tudarmstadt.ukp.dkpro.core/de.tudarmstadt.ukp.dkpro.core.opennlp-model-token-de-maxent/jars/de.tudarmstadt.ukp.dkpro.core.opennlp-model-token-de-maxent-20120616.1.jar!/de/tudarmstadt/ukp/dkpro/core/opennlp/lib/token-de-maxent.properties]
Jul 02, 2014 4:52:50 PM de.tudarmstadt.ukp.dkpro.core.api.resources.ResourceObjectProviderBase loadResource
Information: Producing resource took 257ms
[Wir, brauchen, einen, Aktionsplan, .]
[Aktion, plan]

}}}

== Normalizing a text with !UmlautNormalizer ==

Takes a text and checks for umlauts written as "ae", "oe", or "ue" and normalizes them if they really are umlauts.

{{{

#!/usr/bin/env groovy

@GrabResolver(name='ukp-oss-releases',
      root='http://zoidberg.ukp.informatik.tu-darmstadt.de/artifactory/public-releases')

@Grab(group='de.tudarmstadt.ukp.dkpro.core', version='1.6.1',
     module='de.tudarmstadt.ukp.dkpro.core.umlautnormalizer-asl')

@Grab(group='de.tudarmstadt.ukp.dkpro.core', version='1.6.1',
     module='de.tudarmstadt.ukp.dkpro.core.opennlp-asl')

@Grab(group='de.tudarmstadt.ukp.dkpro.core', version='20121116.0',
     module='de.tudarmstadt.ukp.dkpro.core.umlautnormalizer-model-normalizer-de-default')

import static org.apache.uima.fit.factory.AnalysisEngineFactory.createEngineDescription

import org.apache.uima.analysis_engine.AnalysisEngine
import org.apache.uima.fit.factory.AggregateBuilder
import org.apache.uima.jcas.JCas

import de.tudarmstadt.ukp.dkpro.core.api.metadata.type.DocumentMetaData
import de.tudarmstadt.ukp.dkpro.core.castransformation.ApplyChangesAnnotator
import de.tudarmstadt.ukp.dkpro.core.opennlp.OpenNlpSegmenter
import de.tudarmstadt.ukp.dkpro.core.umlautnormalizer.UmlautNormalizer


def builder = new AggregateBuilder()
builder.add(createEngineDescription(OpenNlpSegmenter))
builder.add(createEngineDescription(UmlautNormalizer))
builder.add(createEngineDescription(ApplyChangesAnnotator), "source",
        "_InitialView", "target", "umlaut_cleaned")

def engine = builder.createAggregate()

def text = "Die Buechsenoeffner koennen oefter benuetzt werden. Neuerscheinungen muss " +
                "der kaeufer kaufen. Schon zum Fruehstueck traf er auf den Maerchenerzaehler, " +
                "den Uebergeek und den Ueberraschungeioeffner. Sein Oeuvre ist beeindruckend."
def jcas = engine.newJCas()
jcas.setDocumentText(text)
jcas.setDocumentLanguage("de")
DocumentMetaData.create(jcas)

engine.process(jcas)

def view = jcas.getView("umlaut_cleaned")
println view.getDocumentText()

}}}

Example output:

{{{
Jul 03, 2014 10:29:49 AM de.tudarmstadt.ukp.dkpro.core.api.resources.ResourceObjectProviderBase info
Information: :: loading settings :: url = jar:file:/usr/share/groovy/lib/ivy.jar!/org/apache/ivy/core/settings/ivysettings.xml
Jul 03, 2014 10:29:49 AM de.tudarmstadt.ukp.dkpro.core.api.resources.ResourceObjectProviderBase loadResource
Information: Producing resource from jar:file:/home/santos/.ivy2/cache/de.tudarmstadt.ukp.dkpro.core/de.tudarmstadt.ukp.dkpro.core.opennlp-upstream-sentence-de-maxent/jars/de.tudarmstadt.ukp.dkpro.core.opennlp-upstream-sentence-de-maxent-20120616.jar!/de/tudarmstadt/ukp/dkpro/core/opennlp/lib/sentence-de-maxent.bin
Jul 03, 2014 10:29:49 AM de.tudarmstadt.ukp.dkpro.core.api.resources.ResourceObjectProviderBase info
Information: :: loading settings :: url = jar:file:/usr/share/groovy/lib/ivy.jar!/org/apache/ivy/core/settings/ivysettings.xml
Jul 03, 2014 10:29:49 AM de.tudarmstadt.ukp.dkpro.core.api.resources.ResourceObjectProviderBase loadResource
Information: Producing resource from jar:file:/home/santos/.ivy2/cache/de.tudarmstadt.ukp.dkpro.core/de.tudarmstadt.ukp.dkpro.core.opennlp-upstream-token-de-maxent/jars/de.tudarmstadt.ukp.dkpro.core.opennlp-upstream-token-de-maxent-20120616.jar!/de/tudarmstadt/ukp/dkpro/core/opennlp/lib/token-de-maxent.bin                                                                                                                                                     
Buechsenoeffner - 0                                                                                                                                                                                                                 
Büchsenöffner - 1732                                                                                                                                                                                                                
koennen - 831456                                                                                                                                                                                                                    
können - 97598630                                                                                                                                                                                                                   
oefter - 24194                                                                                                                                                                                                                      
öfter - 988405                                                                                                                                                                                                                      
benuetzt - 2058                                                                                                                                                                                                                     
benützt - 98690                                                                                                                                                                                                                     
Neuerscheinungen - 905024                                                                                                                                                                                                           
Neürscheinungen - 0                                                                                                                                                                                                                 
kaeufer - 2344                                                                                                                                                                                                                      
käufer - 30327                                                                                                                                                                                                                      
Fruehstueck - 104788                                                                                                                                                                                                                
Frühstück - 2249076                                                                                                                                                                                                                 
Maerchenerzaehler - 310                                                                                                                                                                                                             
Märchenerzähler - 15785                                                                                                                                                                                                             
Uebergeek - 0                                                                                                                                                                                                                       
Übergeek - 0                                                                                                                                                                                                                        
Ueberraschungeioeffner - 0                                                                                                                                                                                                          
Überraschungeiöffner - 0                                                                                                                                                                                                            
Oeuvre - 0                                                                                                                                                                                                                          
Öuvre - 0                                                                                                                                                                                                                           
Jul 03, 2014 10:29:50 AM de.tudarmstadt.ukp.dkpro.core.castransformation.ApplyChangesAnnotator applyChanges(81)                                                                                                                     
Information: Found 10 changes                                                                                                                                                                                                       
Adding from [_InitialView] to [umlaut_cleaned] on [510065709]
Die Büchsenöffner können öfter benützt werden. Neuerscheinungen muss der käufer kaufen. Schon zum Frühstück traf er auf den Märchenerzähler, den Uebergeek und den Ueberraschungeioeffner. Sein Oeuvre ist beeindruckend.

}}}

= Trouble shooting = 

== Out of memory ==

If a script complains about not having enough heap, try the command 
{{{
Linux:    export JAVA_OPTS="-Xmx1g"
}}} 

and then run the script again.

== Verbose dependency resolving ==

Normally, grape resolves dependencies quietly. If a script has many dependencies, that can mean the script may be running for a long time without any visible output on screen, looking like it is hanging. What it really does is downloading the dependencies. To enable verbose output during the dependency resolving phase, set JAVA_OPTS:

{{{
Linux:    export JAVA_OPTS="-Dgroovy.grape.report.downloads=true $JAVA_OPTS"
Windows:  set JAVA_OPTS="-Dgroovy.grape.report.downloads=true %JAVA_OPTS%"
}}}

== Flush caches ==

The scripts download required models and libraries automatically. Sometimes it may be necessary to flush the cache folders. There are two cache folders that you can clear to force re-downloading the dependencies:

   * `~/.groovy/grapes` - dependencies referenced from the Groovy scripts are stored here
   * `~/.ivy2/cache` - models and resources dynamically downloaded by DKPro Core components

If you use Maven for software development, we recommend that you separate the caches (see below). If you did not separate the Groovy cache from the Maven cache, you might also want to consider deleting `~/.m2/repository`.

== Separate Groovy Grape Cache from Maven Cache ==

On some systems, Groovy per default re-uses artifacts that have already been downloaded by Maven. To make sure the Groovy Grape cache is fully separate from the Maven cache, create a file called `grapeConfig.xml` in your `~\.groovy` folder with this content

{{{
<?xml version="1.0"?>
<ivysettings>
    <settings defaultResolver="downloadGrapes"/>
    <resolvers>
        <chain name="downloadGrapes">
            <!-- todo add 'endorsed groovy extensions' resolver here -->
            <filesystem name="cachedGrapes">
                <ivy pattern="${user.home}/.groovy/grapes/[organisation]/[module]/ivy-[revision].xml"/>
                <artifact pattern="${user.home}/.groovy/grapes/[organisation]/[module]/[type]s/[artifact]-[revision].[ext]"/>
            </filesystem>
            <ibiblio name="codehaus" root="http://repository.codehaus.org/" m2compatible="true"/>
            <ibiblio name="ibiblio" m2compatible="true"/>
            <ibiblio name="java.net2" root="http://download.java.net/maven/2/" m2compatible="true"/>
        </chain>
    </resolvers>
</ivysettings>
}}}

Then flush the cache by deleting the folder `~/.groovy/grapes`. Mind that the next time you run a Groovy script, it will take some more time, because the cache needs to be repopulated.

== Groovy 2.3.0 aka _"JCas type used in Java code,  but was not declared in the XML type descriptor"_==

When running our scripts within Groovy 2.3.0, uimaFIT's automatic type detection mechanism does not work. This leads to an error message like this:

{{{
JCas type "de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Token" used in Java code,  
  but was not declared in the XML type descriptor.
}}}

This problem does not occur with Groovy versions 2.1.x or 2.2.x. We reported the [http://jira.codehaus.org/browse/GROOVY-6768 issue] to the Groovy community and hope it will be fixed in the upcoming Groovy 2.3.1.

Thanks to Evan for [http://stackoverflow.com/questions/23504261/dkpro-groovy-usage-and-installation-with-uima reporting] this problem.

== Using SNAPSHOT versions ==

To use SNAPSHOT versions of DKPro Core, add the following line before the first `@Grab` line:

{{{
@GrabResolver(name='ukp-oss-snapshots',
      root='http://zoidberg.ukp.informatik.tu-darmstadt.de/artifactory/public-snapshots')
}}}

Then change the versions of all DKPro Core components to the SNAPSHOT version that you wish to use. It is strongly recommended not to mix versions!